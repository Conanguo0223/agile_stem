{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpy\n",
    "import config\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import random as np_random\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManufacturingProcess:\n",
    "    def __init__(self, env, name, capacity, process_time, conveyor_belt, stats):\n",
    "        \"\"\"\n",
    "        Initializes a manufacturing process.\n",
    "        \n",
    "        Args:\n",
    "            env (simpy.Environment): Simulation environment.\n",
    "            name (str): Name of the process.\n",
    "            capacity (int): Number of machines in the process.\n",
    "            process_time (int or function): Fixed or stochastic processing time.\n",
    "            conveyor_belt (int): Buffer size between stages.\n",
    "            stats (dict): Dictionary to store simulation statistics.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.name = name\n",
    "        self.machine = simpy.Resource(env, capacity)\n",
    "        self.process_time = process_time\n",
    "        self.conveyor_belt = conveyor_belt  # Buffer capacity\n",
    "        self.stats = stats\n",
    "        self.total_busy_time = 0  # Track total processing time\n",
    "\n",
    "        # Store machine utilization per item\n",
    "        self.stats['machine_utilization'][self.name] = 0\n",
    "\n",
    "    def process(self, item):\n",
    "        \"\"\"\n",
    "        Simulates the processing of an item.\n",
    "        \n",
    "        Args:\n",
    "            item (int): The item ID being processed.\n",
    "        \"\"\"\n",
    "        with self.machine.request() as request:\n",
    "            request_start = self.env.now  # Time when machine is requested\n",
    "\n",
    "            yield request  # Wait for machine availability\n",
    "\n",
    "            start_time = self.env.now  # Processing start time\n",
    "            process_duration = self.get_processing_time()\n",
    "            yield self.env.timeout(process_duration)  # Process the item\n",
    "            end_time = self.env.now  # Processing end time\n",
    "\n",
    "            # Track total busy time\n",
    "            self.total_busy_time += process_duration\n",
    "\n",
    "            # Log machine activity\n",
    "            print(f'{self.name} processed item {item} from {start_time} to {end_time} (Duration: {process_duration})')\n",
    "\n",
    "            # Store machine utilization\n",
    "            self.stats['machine_utilization'][self.name] += process_duration\n",
    "\n",
    "    def get_processing_time(self):\n",
    "        \"\"\"\n",
    "        Returns processing time, either fixed or stochastic.\n",
    "        \n",
    "        Returns:\n",
    "            float: Processing time duration.\n",
    "        \"\"\"\n",
    "        if isinstance(self.process_time, (int, float)):  # Fixed processing time\n",
    "            return self.process_time\n",
    "        elif callable(self.process_time):  # Stochastic processing time\n",
    "            return self.process_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REPLICATIONS = 10  # 🔄 Number of replications\n",
    "BASE_SEED = 42         # 🎯 Base seed for reproducibility (change for different overall outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(replication_num, blow_molding_process_time, cleaning_process_time,\n",
    "                    filling_process_time, capping_labeling_process_time, packaging_process_time):\n",
    "    \"\"\"\n",
    "    Runs one replication of the simulation with a unique seed.\n",
    "    \"\"\"\n",
    "    # 🎲 Set a unique seed for each replication\n",
    "    random.seed(BASE_SEED + replication_num)\n",
    "\n",
    "    env = simpy.Environment()\n",
    "\n",
    "    # Statistics tracking for each replication\n",
    "    stats = {\n",
    "        'processed_items': 0,  # Total completed items\n",
    "        'queue_lengths': [],\n",
    "        'bottleneck_station': None,\n",
    "        'machine_utilization': {}\n",
    "    }\n",
    "\n",
    "    # Define manufacturing stages\n",
    "    blow_molding = ManufacturingProcess(env, \"Blow Molding\", config.MACHINE_CAPACITIES[0],\n",
    "                                        blow_molding_process_time, config.CONVEYOR_CAPACITIES[0], stats)\n",
    "    cleaning = ManufacturingProcess(env, \"Cleaning\", config.MACHINE_CAPACITIES[1],\n",
    "                                    cleaning_process_time, config.CONVEYOR_CAPACITIES[1], stats)\n",
    "    filling = ManufacturingProcess(env, \"Filling\", config.MACHINE_CAPACITIES[2],\n",
    "                                   filling_process_time, config.CONVEYOR_CAPACITIES[2], stats)\n",
    "    capping_labeling = ManufacturingProcess(env, \"Capping & Labeling\", config.MACHINE_CAPACITIES[3],\n",
    "                                            capping_labeling_process_time, config.CONVEYOR_CAPACITIES[3], stats)\n",
    "    packaging = ManufacturingProcess(env, \"Packaging\", config.MACHINE_CAPACITIES[4],\n",
    "                                    packaging_process_time, config.CONVEYOR_CAPACITIES[4], stats)\n",
    "\n",
    "    machines = [blow_molding, cleaning, filling, capping_labeling, packaging]\n",
    "\n",
    "    def get_demand_interval():\n",
    "        \"\"\"Returns interarrival time for new items (demand rate).\"\"\"\n",
    "        if isinstance(config.DEMAND_RATE, (int, float)):\n",
    "            return config.DEMAND_RATE\n",
    "        elif callable(config.DEMAND_RATE):\n",
    "            return config.DEMAND_RATE()\n",
    "\n",
    "    def production_line(env):\n",
    "        \"\"\"Simulates item arrivals and processing through all manufacturing stages.\"\"\"\n",
    "        item = 1\n",
    "        # First item arrives at time 0\n",
    "        print(f'Item {item} enters the system at time {env.now:.5f}')\n",
    "        env.process(blow_molding.process(item))\n",
    "        env.process(cleaning.process(item))\n",
    "        env.process(filling.process(item))\n",
    "        env.process(capping_labeling.process(item))\n",
    "        env.process(packaging.process(item))\n",
    "        stats['processed_items'] += 1\n",
    "\n",
    "        next_arrival_time = get_demand_interval()\n",
    "\n",
    "        while env.now < config.SIM_TIME:\n",
    "            yield env.timeout(max(0, next_arrival_time - env.now))\n",
    "\n",
    "            item += 1\n",
    "            print(f'Item {item} enters the system at time {env.now:.5f}')\n",
    "            env.process(blow_molding.process(item))\n",
    "            env.process(cleaning.process(item))\n",
    "            env.process(filling.process(item))\n",
    "            env.process(capping_labeling.process(item))\n",
    "            env.process(packaging.process(item))\n",
    "\n",
    "            stats['processed_items'] += 1\n",
    "            next_arrival_time += get_demand_interval()\n",
    "\n",
    "    # Start and run simulation\n",
    "    env.process(production_line(env))\n",
    "    until = config.SIM_TIME  # Run for 90% of the simulation time\n",
    "    while env.peek() < until:\n",
    "        print(f\"Simulation time: {env.now:.5f}\")\n",
    "        print(f\"Processed items: {env}\")\n",
    "        env.step()\n",
    "    # env.run(until=config.SIM_TIME)\n",
    "\n",
    "    # Compute key performance metrics\n",
    "    total_time = config.SIM_TIME\n",
    "    throughput = stats['processed_items'] / (total_time / 60)  # Bottles per hour\n",
    "    avg_queue_length = (sum(stats['queue_lengths']) / len(stats['queue_lengths'])) if stats['queue_lengths'] else 0\n",
    "    bottleneck_station = max(stats['queue_lengths']) if stats['queue_lengths'] else 0\n",
    "    machine_utilization = {\n",
    "        machine.name: (machine.total_busy_time / total_time) * 100 for machine in machines\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"total_bottles\": stats['processed_items'],\n",
    "        \"throughput\": throughput,\n",
    "        \"avg_queue_length\": avg_queue_length,\n",
    "        \"max_queue_length\": bottleneck_station,\n",
    "        \"utilization\": machine_utilization>\n",
    "    } # End of run_simulation function  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for i in range(1):\n",
    "    print(f\"\\n=== Running Simulation Replication {i+1} ===\")\n",
    "    result = run_simulation(i)\n",
    "    all_results.append(result)\n",
    "\n",
    "# 📊 Aggregate results across replications\n",
    "total_bottles_list = [res[\"total_bottles\"] for res in all_results]\n",
    "throughput_list = [res[\"throughput\"] for res in all_results]\n",
    "avg_queue_list = [res[\"avg_queue_length\"] for res in all_results]\n",
    "max_queue_list = [res[\"max_queue_length\"] for res in all_results]\n",
    "\n",
    "# 🧮 Compute mean and standard deviation\n",
    "mean_bottles, std_bottles = np.mean(total_bottles_list), np.std(total_bottles_list)\n",
    "mean_throughput, std_throughput = np.mean(throughput_list), np.std(throughput_list)\n",
    "mean_queue, std_queue = np.mean(avg_queue_list), np.std(avg_queue_list)\n",
    "mean_max_queue, std_max_queue = np.mean(max_queue_list), np.std(max_queue_list)\n",
    "\n",
    "# 🏭 Machine utilization (mean ± std dev)\n",
    "utilization_results = {machine: [] for machine in all_results[0][\"utilization\"].keys()}\n",
    "for res in all_results:\n",
    "    for machine, util in res[\"utilization\"].items():\n",
    "        utilization_results[machine].append(util)\n",
    "\n",
    "# 📢 Print aggregated results\n",
    "print(\"\\n=== Aggregated Simulation Results Across Replications ===\")\n",
    "print(f\"Total Bottles Produced: Mean = {mean_bottles:.2f}, Std Dev = {std_bottles:.2f}\")\n",
    "print(f\"Throughput: Mean = {mean_throughput:.2f} bottles/hour, Std Dev = {std_throughput:.2f}\")\n",
    "print(f\"Average Queue Length: Mean = {mean_queue:.2f}, Std Dev = {std_queue:.2f}\")\n",
    "print(f\"Maximum Queue Length: Mean = {mean_max_queue:.2f}, Std Dev = {std_max_queue:.2f}\")\n",
    "\n",
    "print(\"\\nMachine Utilization (Mean ± Std Dev):\")\n",
    "for machine, values in utilization_results.items():\n",
    "    print(f\"  {machine}: {np.mean(values):.2f}% ± {np.std(values):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "NUM_MACHINES = 5\n",
    "MACHINE_CAPACITIES = [10, 8, 12, 6, 15]  # Capacities for each machine\n",
    "PROCESS_TIMES = [2, 1.5, 3, 2.5, 4]  # Processing times for each machine (in simulation time units)\n",
    "INTER_ARRIVAL_TIME = 1  # Time between arrivals of unit plastics\n",
    "\n",
    "# Machine names for clarity\n",
    "MACHINE_NAMES = [\"Blow Molding\", \"Cleaning\", \"Filling\", \"Capping/Labeling\", \"Packaging\"]\n",
    "\n",
    "def unit_plastic_generator(env, machines):\n",
    "    \"\"\"Generates unit plastics periodically and sends them to the first machine.\"\"\"\n",
    "    unit_id = 0\n",
    "    while True:\n",
    "        yield env.timeout(random.expovariate(1 / INTER_ARRIVAL_TIME))\n",
    "        print(f\"Time {env.now}: Unit Plastic {unit_id} created.\")\n",
    "        env.process(process_unit(env, f\"Unit Plastic {unit_id}\", machines))\n",
    "        unit_id += 1\n",
    "\n",
    "def process_unit(env, unit_name, machines):\n",
    "    \"\"\"Processes a unit plastic through the pipeline.\"\"\"\n",
    "    for i, machine in enumerate(machines):\n",
    "        with machine.request() as req:\n",
    "            yield req\n",
    "            print(f\"Time {env.now}: {unit_name} starts at {MACHINE_NAMES[i]}.\")\n",
    "            yield env.timeout(random.normalvariate(PROCESS_TIMES[i], PROCESS_TIMES[i] * 0.1))  # Add variability\n",
    "            print(f\"Time {env.now}: {unit_name} finishes at {MACHINE_NAMES[i]}.\")\n",
    "\n",
    "# Create the simulation environment\n",
    "env = simpy.Environment()\n",
    "\n",
    "# Create resources (machines) with defined capacities\n",
    "machines = [simpy.Resource(env, capacity=MACHINE_CAPACITIES[i]) for i in range(NUM_MACHINES)]\n",
    "\n",
    "# Start the unit plastic generator process\n",
    "env.process(unit_plastic_generator(env, machines))\n",
    "\n",
    "# Run the simulation for a specified duration\n",
    "SIMULATION_TIME = 50\n",
    "env.run(until=SIMULATION_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(BASE_SEED + replication_num)\n",
    "\n",
    "env = simpy.Environment()\n",
    "\n",
    "# Statistics tracking for each replication\n",
    "stats = {\n",
    "    'processed_items': 0,  # Total completed items\n",
    "    'queue_lengths': [],\n",
    "    'bottleneck_station': None,\n",
    "    'machine_utilization': {}\n",
    "}\n",
    "\n",
    "# Define manufacturing stages\n",
    "blow_molding = ManufacturingProcess(env, \"Blow Molding\", config.MACHINE_CAPACITIES[0],\n",
    "                                    blow_molding_process_time, config.CONVEYOR_CAPACITIES[0], stats)\n",
    "cleaning = ManufacturingProcess(env, \"Cleaning\", config.MACHINE_CAPACITIES[1],\n",
    "                                cleaning_process_time, config.CONVEYOR_CAPACITIES[1], stats)\n",
    "filling = ManufacturingProcess(env, \"Filling\", config.MACHINE_CAPACITIES[2],\n",
    "                                filling_process_time, config.CONVEYOR_CAPACITIES[2], stats)\n",
    "capping_labeling = ManufacturingProcess(env, \"Capping & Labeling\", config.MACHINE_CAPACITIES[3],\n",
    "                                        capping_labeling_process_time, config.CONVEYOR_CAPACITIES[3], stats)\n",
    "packaging = ManufacturingProcess(env, \"Packaging\", config.MACHINE_CAPACITIES[4],\n",
    "                                packaging_process_time, config.CONVEYOR_CAPACITIES[4], stats)\n",
    "\n",
    "machines = [blow_molding, cleaning, filling, capping_labeling, packaging]\n",
    "\n",
    "def get_demand_interval():\n",
    "    \"\"\"Returns interarrival time for new items (demand rate).\"\"\"\n",
    "    if isinstance(config.DEMAND_RATE, (int, float)):\n",
    "        return config.DEMAND_RATE\n",
    "    elif callable(config.DEMAND_RATE):\n",
    "        return config.DEMAND_RATE()\n",
    "\n",
    "def production_line(env):\n",
    "    \"\"\"Simulates item arrivals and processing through all manufacturing stages.\"\"\"\n",
    "    item = 1\n",
    "    # First item arrives at time 0\n",
    "    print(f'Item {item} enters the system at time {env.now:.5f}')\n",
    "    env.process(blow_molding.process(item))\n",
    "    env.process(cleaning.process(item))\n",
    "    env.process(filling.process(item))\n",
    "    env.process(capping_labeling.process(item))\n",
    "    env.process(packaging.process(item))\n",
    "    stats['processed_items'] += 1\n",
    "\n",
    "    next_arrival_time = get_demand_interval()\n",
    "\n",
    "    while env.now < config.SIM_TIME:\n",
    "        yield env.timeout(max(0, next_arrival_time - env.now))\n",
    "\n",
    "        item += 1\n",
    "        print(f'Item {item} enters the system at time {env.now:.5f}')\n",
    "        env.process(blow_molding.process(item))\n",
    "        env.process(cleaning.process(item))\n",
    "        env.process(filling.process(item))\n",
    "        env.process(capping_labeling.process(item))\n",
    "        env.process(packaging.process(item))\n",
    "\n",
    "        stats['processed_items'] += 1\n",
    "        next_arrival_time += get_demand_interval()\n",
    "\n",
    "# Start and run simulation\n",
    "env.process(production_line(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        # actor\n",
    "        if has_continuous_action_space :\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "\n",
    "        \n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        \n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        \n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "        \n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# logging file\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpy\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Define constants for simulation\n",
    "NUM_REPLICATIONS = 10  # Number of replications for evaluation\n",
    "BASE_SEED = 42         # Base seed for reproducibility\n",
    "SIM_TIME = 100         # Simulation time per episode\n",
    "\n",
    "# RL Environment for PPO Agent\n",
    "class ManufacturingRL(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ManufacturingRL, self).__init__()\n",
    "        \n",
    "        # Observation space: Queue lengths and machine utilization (normalized)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n",
    "        \n",
    "        # Action space: Adjust machine processing times (continuous values)\n",
    "        self.action_space = spaces.Box(low=0.5, high=5.0, shape=(5,), dtype=np.float32)\n",
    "        \n",
    "        # Initialize SimPy environment and manufacturing processes\n",
    "        self.env = simpy.Environment()\n",
    "        self.stats = {\n",
    "            'processed_items': 0,\n",
    "            'queue_lengths': [],\n",
    "            'machine_utilization': {}\n",
    "        }\n",
    "        \n",
    "        # Machine parameters: capacities and processing times\n",
    "        self.machine_names = [\"Blow Molding\", \"Cleaning\", \"Filling\", \"Capping & Labeling\", \"Packaging\"]\n",
    "        self.machines = []\n",
    "        self.processing_times = [2.0] * 5  # Initial processing times\n",
    "        \n",
    "        for i in range(5):\n",
    "            self.machines.append(\n",
    "                ManufacturingProcess(self.env, self.machine_names[i], capacity=1,\n",
    "                                     processing_time=self.processing_times[i],\n",
    "                                     conveyor_capacity=10, stats=self.stats)\n",
    "            )\n",
    "        \n",
    "        self.current_time = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment for a new episode.\"\"\"\n",
    "        self.env = simpy.Environment()\n",
    "        self.stats = {\n",
    "            'processed_items': 0,\n",
    "            'queue_lengths': [],\n",
    "            'machine_utilization': {}\n",
    "        }\n",
    "        \n",
    "        self.machines = []\n",
    "        for i in range(5):\n",
    "            self.machines.append(\n",
    "                ManufacturingProcess(self.env, self.machine_names[i], capacity=1,\n",
    "                                     processing_time=self.processing_times[i],\n",
    "                                     conveyor_capacity=10, stats=self.stats)\n",
    "            )\n",
    "        \n",
    "        self.current_time = 0\n",
    "        \n",
    "        return np.zeros(5)  # Initial observation (normalized values)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Performs one step in the simulation.\"\"\"\n",
    "        # Update processing times based on action taken by PPO agent\n",
    "        for i in range(5):\n",
    "            self.machines[i].processing_time = action[i]\n",
    "        \n",
    "        # Run one timestep in SimPy environment\n",
    "        self.env.run(until=self.current_time + 1)\n",
    "        \n",
    "        # Compute observations (normalized queue lengths and utilization)\n",
    "        queue_lengths = [len(machine.queue.items) / 10 for machine in self.machines]\n",
    "        utilization = [machine.total_busy_time / SIM_TIME for machine in self.machines]\n",
    "        \n",
    "        observation = np.array(queue_lengths + utilization[:5])\n",
    "        \n",
    "        # Compute reward based on throughput and queue lengths (example reward function)\n",
    "        throughput = self.stats['processed_items'] / SIM_TIME\n",
    "        reward = throughput - sum(queue_lengths) * 0.1\n",
    "        \n",
    "        done = self.current_time >= SIM_TIME\n",
    "        \n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Optional render method.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Define ManufacturingProcess class (simplified for this example)\n",
    "class ManufacturingProcess:\n",
    "    def __init__(self, env, name, capacity, processing_time, conveyor_capacity, stats):\n",
    "        \"\"\"\n",
    "        Initializes a manufacturing process with a queue and resource.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.name = name\n",
    "        self.capacity = capacity  # Machine capacity (number of items it can process simultaneously)\n",
    "        self.processing_time = processing_time  # Time required to process one item\n",
    "        self.conveyor_capacity = conveyor_capacity  # Maximum queue length\n",
    "        self.stats = stats  # Shared statistics dictionary\n",
    "        \n",
    "        # SimPy resources for machine and conveyor (queue)\n",
    "        self.machine = simpy.Resource(env, capacity=self.capacity)\n",
    "        self.queue = simpy.Store(env, capacity=self.conveyor_capacity)  # Queue with limited capacity\n",
    "        \n",
    "        # Tracking utilization\n",
    "        self.total_busy_time = 0.0  # Total time machine is busy\n",
    "        self.last_start_time = None  # For utilization calculation\n",
    "\n",
    "    def process(self, item):\n",
    "        \"\"\"\n",
    "        Processes an item through the machine.\n",
    "        Items wait in the queue if the machine is busy.\n",
    "        \"\"\"\n",
    "        # Add item to the queue\n",
    "        yield self.queue.put(item)\n",
    "        \n",
    "        print(f\"Time {self.env.now:.2f}: Item {item} enters {self.name} queue (Queue Length: {len(self.queue.items)}).\")\n",
    "        \n",
    "        # Wait for machine availability\n",
    "        with self.machine.request() as req:\n",
    "            yield req  # Wait until machine is free\n",
    "            \n",
    "            # Remove item from the queue\n",
    "            yield self.queue.get()\n",
    "            \n",
    "            print(f\"Time {self.env.now:.2f}: Item {item} starts processing at {self.name}.\")\n",
    "            \n",
    "            # Track busy time for utilization calculation\n",
    "            self.last_start_time = self.env.now\n",
    "            \n",
    "            # Simulate processing time\n",
    "            yield self.env.timeout(self.processing_time)\n",
    "            \n",
    "            print(f\"Time {self.env.now:.2f}: Item {item} finishes processing at {self.name}.\")\n",
    "            \n",
    "            # Update total busy time\n",
    "            self.total_busy_time += self.env.now - self.last_start_time\n",
    "            \n",
    "            # Track queue length for statistics\n",
    "            self.stats['queue_lengths'].append(len(self.queue.items))\n",
    "\n",
    "\n",
    "# Train PPO Agent on ManufacturingRL Environment\n",
    "def train_ppo_agent():\n",
    "    env = ManufacturingRL()\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    \n",
    "    print(\"\\n=== Training PPO Agent ===\")\n",
    "    model.learn(total_timesteps=10000)  # Train for a fixed number of timesteps\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluate trained PPO Agent on multiple replications\n",
    "def evaluate_agent(model):\n",
    "    env = ManufacturingRL()\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n=== Evaluating PPO Agent ===\")\n",
    "    for _ in range(NUM_REPLICATIONS):\n",
    "        obs = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, _info = env.step(action)\n",
    "        \n",
    "        results.append(env.stats['processed_items'])\n",
    "    \n",
    "    print(f\"Average Bottles Produced: {np.mean(results):.2f}\")\n",
    "    print(f\"Standard Deviation: {np.std(results):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conangg/miniconda3/envs/agile_stem/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/conangg/miniconda3/envs/agile_stem/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training PPO Agent ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10,) into shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 155\u001b[0m, in \u001b[0;36mtrain_ppo_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training PPO Agent ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train for a fixed number of timesteps\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/agile_stem/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/agile_stem/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 324\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/agile_stem/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/agile_stem/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/agile_stem/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:72\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     71\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[0;32m~/miniconda3/envs/agile_stem/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:109\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[0;34m(self, env_idx, obs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuf_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs[key]\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (10,) into shape (5,)"
     ]
    }
   ],
   "source": [
    "ppo_model = train_ppo_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agile_stem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
